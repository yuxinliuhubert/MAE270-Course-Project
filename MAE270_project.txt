\title{
MAE270A Project
}

\section*{Prof. M'Closkey}

\section*{Due date: 5pm, Dec. 13, 2024}

\section*{Guidelines}

Submit a PDF report organized according to the "tasks" outlined in this project description. The main results are shown in figures, however, write a short narrative for each task and answer any questions posed in the project description. You can choose up to one partner for this project and only a single report needs to be submitted. Clearly note yourself and partner on the report cover sheet. The Matlab code that you write to complete each task must be included in the report appendix. The code must be clearly commented.

\section*{Project objectives}
- Learn about sample-and-hold models of linear systems.
- Identify empirical frequency responses from input-output data sequences.
- Estimating discrete-time state-space models from the pulse response estimate.
- Application to a three-input/two-output physical system created by connecting "subsystems" in specific a topology.

\section*{1 Discrete-time systems}

\subsection*{1.1 Pulse response of discrete-time system}

Consider a multi-input/multi-output, discrete-time linear system with $m$ outputs and $q$ inputs,
$$
\begin{align*}
\mathbf{x}[k+1] & =A \mathbf{x}[k]+B \mathbf{u}[k] \\
\mathbf{y}[k] & =C \mathbf{x}[k] \tag{1}
\end{align*}
$$
where the state dimension is $n_{s}$ so $A \in \mathbf{R}^{n_{s} \times n_{s}}, B \in \mathbf{R}^{n_{s} \times q}$ and $C \in \mathbf{R}^{m \times n_{s}}$. It is assumed the "feedthrough" matrix $D \in \mathbf{R}^{m \times q}$ is zero. This system may have been derived from the testing of a continuous-time system as noted in Sec. 1.2, or, it may be a digital filter that is implemented within signal processing equipment.

We will assume the state-space matrices have real elements. It is useful to refer to the individual "channels" of the input $\mathbf{u}[k]$ as follows,
$$
\mathbf{u}[k]=\left[\begin{array}{c}
u_{1}[k] \\
u_{2}[k] \\
\vdots \\
u_{q}[k]
\end{array}\right]
$$

Define the discrete-time unit pulse as the scalar-valued signal
$$
\delta[k]= \begin{cases}1 & k=0 \\ 0 & k>0\end{cases}
$$

Using this notation, the $r$ th column of the pulse response sequence $\{h[k]\}$, where $h[k] \in \mathbf{R}^{m \times q}$, is obtained when $\mathbf{x}[0]=0$ and the input is given by
$$
\begin{aligned}
& u_{p}=0, p \neq r \\
& u_{r}=\delta,
\end{aligned}
$$

The pulse response is the discrete-time analog of the impulse response of a continuous-time system. It is straightforward to show
$$
\begin{align*}
& h[0]=0 \in \mathbf{R}^{m \times q} \\
& h[k]=C A^{k-1} B \in \mathbf{R}^{m \times q}, k=1,2,3, \ldots \tag{2}
\end{align*}
$$

\subsection*{1.2 Sampling a continuous-time state-space system}

This section shows how "sampling" a continuous-time system produces a discrete-time system. Consider testing a continuous-time system, denoted "System", in Fig. 1. The discrete-time input sequence is denoted $\mathbf{u}[k], k \in \mathbb{Z}$. In practice, $\mathbf{u}[k]$ is generated by signal processing hardware for some finite range of $k$ and is generally under control of the engineer ${ }^{1}$. In modern test equipment the discrete-time signal $\mathbf{u}[k]$ is converted into a continuous-time signal $\mathbf{u}(t)$ by a digital-to-analog convertor (DAC). The DAC implements a zero-order hold which extends the value of the $\mathbf{u}[k]$ over the time interval $\left[k t_{s},(k+1) t_{s}\right)$. In other words, $\mathbf{u}(t)=\mathbf{u}[k], t \in\left[k t_{s},(k+1) t_{s}\right)$. In the pulse response testing discussed in Sec. 1.1, $k=0$ refers to the sample at which the input pulse occurs. The sample period is denoted $t_{s}$. Let $p(t)$ represent a continuous-time rectangular pulse of height " 1 " and width $t_{s}$,
$$
p(t)= \begin{cases}1 & t \in\left[0, t_{s}\right]  \tag{3}\\ 0 & t \notin\left[0, t_{s}\right]\end{cases}
$$

Then, the continuous-time input to the system can be expressed
$$
\begin{equation*}
\mathbf{u}(t)=\sum_{k=-\infty}^{\infty} \mathbf{u}[k] p\left(t-k t_{s}\right) \tag{4}
\end{equation*}
$$

Thus, the continuous-time input $\mathbf{u}(t)$ is exactly known for all $t$ and can be determined from $\mathbf{u}[k]$.
The continuous-time signal $\mathbf{y}(t)$ is the output of the system. This signal is converted to a discretetime signal through the process of sampling. In practice, sampling is implemented with a integrated circuit called an analog-to-digital convertor (ADC). A trigger signal controls the ADC and tells it when to capture a sample of the continuous time signal. Thus, if the ADC is triggered on the time grid $\left\{0, t_{s}, 2 t_{s}, 3 t_{s}, \ldots\right\}$, then a discrete-time output sequence is defined as $\mathbf{y}[k]=\mathbf{y}\left(k t_{s}\right), k=0,1,2,3, \ldots$

\footnotetext{
${ }^{1}$ The square brackets in $\mathbf{u}[k]$ denote it is a discrete-time signal and the sample index is determined by an integer $k$. This notation may seem more cumbersome, however, we will need to refer to individual elements of $\mathbf{u}[k]$ and using subscripts for this purpose makes more sense
}
![](https://cdn.mathpix.com/cropped/2024_11_25_befee36b63c0601e103eg-03.jpg?height=578&width=1654&top_left_y=340&top_left_x=203)

Figure 1: Testing a system with modern signal processing equipment.

It is assumed that the DAC and ADC are synchronized so that $\mathbf{u}[k]$ and $\mathbf{y}[k]$ are associated with same point in time, i.e. $k t_{s}$.

Suppose the continuous-time system is described by linear, time-invariant state-space equations
$$
\begin{align*}
& \dot{\mathbf{x}}=A_{c} \mathbf{x}+B_{c} \mathbf{u}  \tag{5}\\
& \mathbf{y}=C_{c} \mathbf{x}+D_{c} \mathbf{u} .
\end{align*}
$$

The subscript " $c$ " just denotes these state-space data are associated with the continuous-time system. Given the block diagram in Fig. 1, it is possible to determine a discrete-time state-space system that exactly relates the input and output samples $\mathbf{u}[k]$ and $\mathbf{y}[k]$. To determine the difference equation it is sufficient to relate the values of $\mathbf{x}$ at $t=k t_{s}$ and $t=(k+1) t_{s}$. This is accomplished using the closed-form solution that was derived in the notes,
$$
\mathbf{x}[k+1]=e^{A_{c} t_{s}} \mathbf{x}[k]+\int_{k t_{s}}^{(k+1) t_{s}} e^{A_{c}\left((k+1) t_{s}-\tau\right)} B_{c} \mathbf{u}(\tau) d \tau
$$
where $\mathbf{x}[k]=\mathbf{x}\left(k t_{s}\right)$. Changing variables in the integral yields,
$$
\mathbf{x}[k+1]=e^{A_{c} t_{s}} \mathbf{x}[k]+\int_{0}^{t_{s}} e^{A_{c}\left(t_{s}-\tau\right)} B_{c} \mathbf{u}\left(\tau+k t_{s}\right) d \tau
$$

Note $\mathbf{u}\left(\tau+k t_{s}\right)=\mathbf{u}[k], \tau \in\left[0, t_{s}\right]$, so
$$
\begin{aligned}
\mathbf{x}[k+1] & =e^{A_{c} t_{s}} \mathbf{x}[k]+\int_{0}^{t_{s}} e^{A_{c}\left(t_{s}-\tau\right)} B \mathbf{u}[k] d \tau \\
& =e^{A_{c} t_{s}} \mathbf{x}[k]+\int_{0}^{t_{s}} e^{A_{c}\left(t_{s}-\tau\right)} B_{c} d \tau \mathbf{u}[k] \\
& =e^{A_{c} t_{s}} \mathbf{x}[k]-A_{c}^{-1}\left(I-e^{A_{c} t_{s}}\right) B_{c} \mathbf{u}[k]
\end{aligned}
$$

It is assumed $A_{c}$ is invertible (no zero eigenvalues). If this is not the case then the integral must be determined another way. Thus, the discrete-time equations are
$$
\begin{align*}
\mathbf{x}[k+1] & =A \mathbf{x}[k]+B \mathbf{u}[k]  \tag{6}\\
\mathbf{y}[k] & =C \mathbf{x}[k]+D \mathbf{u}[k]
\end{align*}
$$
where
$$
\begin{equation*}
A=e^{A_{c} t_{s}}, \quad B=-A_{c}^{-1}\left(I-e^{A_{c} t_{s}}\right) B_{c}, \quad C=C_{c}, \quad D=D_{c} . \tag{7}
\end{equation*}
$$

The frequency response of the discrete-time system is given by
$$
H[\omega]=C\left(e^{j \omega t_{s}} I-A\right)^{-1} B+D
$$

Although not derived, $H[\omega]$ is the discrete-time Fourier transform (assuming asymptotically stability) of the system's pulse response given by (2).

\section*{2 Empirical frequency response estimates}

Empirical frequency response estimates (EFRE) are a non-parametric representation of the inputoutput behavior of a system. EFRE can be obtained in a variety of ways:
1. Sinewave testing builds up a frequency-by-frequency estimate of the system's frequency response (the test inputs are sinusoids).
2. Impulse testing is common in modal analysis: an impact hammer applies a pulse-like force to the structure under test; the structure response is measured (typically with accelerometers) and the Fourier transforms of the output measurements are normalized by the Fourier transform of the input pulse to yield multi-frequency estimates of the frequency response.
3. Testing with "persistent" inputs like random signals, chirps, binary sequences, etc. to which spectral estimation techniques are applied to eventually recover the frequency response.

There is no one "best" experimental technique to obtain an EFRE.

\subsection*{2.1 Spectral density estimation from measurement data}

EFRE using "random" test inputs will be brief discussed. There are a many interesting details that are skipped. Suppose a linear system is subjected to a persistent input, $u$ (the single-input/single-output continuous-time case is considered). The output is $y=h * u$, where $h$ is the system's impulse response and " $*$ " denotes convolution. If $u$ is bounded, as it is in any practical scenario, then $y$ is bounded when the system is asymptotically stable. Correlation functions are defined for these signals:
$$
\left.\begin{array}{rl}
\text { Auto-correlations: } & R_{u u}(t)
\end{array}=\lim _{T \rightarrow \infty} \frac{1}{T} \int_{-T / 2}^{T / 2} u(\tau) u^{*}(\tau-t) d \tau\right]
$$
(the superscript $u^{*}$ denotes the complex-conjugate, not convolution). These definitions may be applied to a wide class of persistent signals. For non-periodic "random" signals, it is assumed that the signal
properties are translation invariant. In other words, the absolute time is of no consequence in the formula, only the relative lag between the signals in the integrand is important.

An interesting and useful property of linear systems is the following:
$$
\begin{equation*}
R_{y u}=h * R_{u u} \tag{9}
\end{equation*}
$$

In other words, if the signal $R_{u u}$ is applied as an input, then the output is $R_{y u}$. This relation is the basis for determining the system's impulse response from test data generated with random inputs. For example, if $R_{u u}$ is impulse-like, then $R_{y u}$ is an estimate of the system's impulse response. Computing the Fourier transform of (9) yields,
$$
S_{y u}=\hat{h} S_{u u}
$$
where $S_{u u}$ is the Fourier transform of $R_{u u}, S_{y u}$ is the Fourier transform of $R_{y u}$, and $\hat{h}$ is the Fourier transform of the system's impulse response, i.e. the frequency response. These transforms are the mean-square auto-spectral density of $u$ and cross-spectral density of $y$ and $u$, respectively. These are also called power spectral densities. Note that the frequency response is equal to,
$$
\begin{equation*}
\hat{h}=S_{y u} S_{u u}^{-1} \tag{10}
\end{equation*}
$$

The ratio is formed only when $S_{u u}$ has sufficient power at a given frequency. This is the basis for estimating a system's frequency response from test data:
1. apply a random input to the system
2. gather enough data to reliably estimate $R_{y u}$ and $R_{u u}$
3. compute the Fourier transforms of $R_{y u}$ and $R_{u u}$ to get the power spectral densities $S_{y u}$ and $S_{u u}$
4. estimate $\hat{h}$ from (10)

It is fair to ask why not simply use an impulsive input so that $y \approx h$. The answer is that the measurement $y$ typically contains "noise" and effects of other disturbances acting on the system. Thus, a more realistic model for the signal $y$ is
$$
y=h * u+n
$$
where $n$ captures the presence of noise and disturbances. In this case,
$$
R_{y u}=h * R_{u u}+R_{n u}
$$

In most cases, we can arrange $R_{n u}=0$, that is, the noise and input are uncorrelated so the relation (9) still holds even in the presence of disturbances and noise. This is not the case if we selected $u$ to be impulsive because there is no averaging to remove the effects of disturbances. Furthermore, there are limitations on the amplitude of input signals that can be physically applied to a system so in practice an impulsive input will be limited in energy and consequently may produce a "weak" response. This limitation is removed when a persistent test input is selected.

There is one more innovation that occurred in the 1960's with the advent of inexpensive signal processing equipment ${ }^{2}: S_{u u}$ and $S_{y u}$ are directly estimated, therefore bypassing the correlation estimates. In other words, the frequency response is estimated using the following modified procedure:

\footnotetext{
${ }^{2}$ A revolution in signal processing occurred in the 1960's when the fast Fourier transform (FFT) algorithm was discovered/rediscovered and digital computers based on integrated circuits where becoming more widely distributed. This allowed the "real-time" estimation of spectral densities with special-purpose test equipment.
}
1. apply a random input to the system
2. gather enough data to reliably estimate $S_{y u}$ and $S_{u u}$
3. estimate $\hat{h}$ from (10)

\subsection*{2.2 Pulse response estimates}

In sampled-data systems, the test data are records of discrete-time input-output data. Thus, the power spectral densities of these discrete-time signals are computed. Matlab has functions which estimate power spectral densities of discrete-time signals. The function used in this project is cpsd. Suppose scalar-valued time series are given by the Matlab variables $u$ and y , which are the input and output date vectors, respectively. Let fs , win and nfft represent the sample frequency associated with the data, the data-tapering window and length (number of points) of the data sub-record upon which the spectral estimates are based. Then, the following Matlab code estimates $S_{u u}$ and $S_{y u}$ on a specific grid of frequencies:
```
ts = 1/50; % sample period in seconds
fs = 1/ts; % sample rate in hertz
nfft = 250; % sub-record length nfft/fs = 5 seconds
win = hamming(nfft); % use Hamming data tapering window
[Suu,f] = cpsd(u,u,win,[],nfft,fs,'twosided'); % auto-spectral density of u
[Syu,f] = cpsd(y,u,win,[],nfft,fs,'twosided'); % cross-spectral density of y and u
```

The system's frequency response is then estimated on the frequency grid associated with the spectral densities (the f variable returned from cpsd) by simply forming the ratio $\mathrm{H}=\mathrm{Syu} . / \mathrm{Suu}$. The twosided option is convenient because H is determined on a frequency grid that is compatible with the inverse fast Fourier transform implemented in Matlab. Thus, an estimate of system's pulse response is simply $\mathrm{h}=\operatorname{ifft}(\mathrm{H})$. To conclude, a simple way to recover an estimate of the discrete-time system's pulse response is to follow these steps:
1. Apply a random input to the system and gather sufficient data for low-variance estimates of the spectral densities.
2. Suppose the data vectors are denoted $u$ and $y$ in Matlab (assumed to be a single-input/singleoutput system for now). Estimate the power spectra according to
[Suu,f] = cpsd(u,u,win, [],nfft,fs,'twosided');
[Syu,f] = cpsd(y,u,win, [],nfft,fs,'twosided');
3. Estimate the frequency response H = Syu./Suu.
4. Estimate the pulse response $h=\operatorname{ifft}(\mathrm{H})$ with associated time vector $t=[0: 1 \mathrm{length}(\mathrm{h})-1] / \mathrm{fs}$;

The next section shows how to estimate a state-space model from the pulse response.

\section*{3 Time-domain model identification from pulse response estimates}

The discrete-time system (1) has associated observability and controllability matrices,
$$
\mathcal{O}_{n}=\left[\begin{array}{c}
C \\
C A \\
C A^{2} \\
\vdots \\
C A^{n_{s}-1}
\end{array}\right] \in \mathbf{R}^{m n_{s} \times n_{s}}, \quad \mathcal{C}_{n}=\left[\begin{array}{lllll}
B & A B & A^{2} B & \cdots & A^{n_{s}-1} B
\end{array}\right] \in \mathbf{R}^{n_{s} \times n_{s} q},
$$
which are assumed to be full rank, i.e. the model is controllable and observable. The system has $m$ outputs and $q$ inputs.

A finite number of terms from the pulse response sequence can be organized into an $m n \times n q$ Hankel matrix, denoted $M_{n}$,
$$
M_{n}=\left[\begin{array}{ccccc}
h[1] & h[2] & h[3] & \cdots & h[n]  \tag{11}\\
h[2] & h[3] & h[4] & \cdots & h[n+1] \\
h[3] & h[4] & h[5] & \cdots & h[n+2] \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
h[n] & h[n+1] & h[n+2] & \cdots & h[2 n-1]
\end{array}\right] \in \mathbf{R}^{m n \times n q} .
$$

This matrix uses $2 n-1$ samples of the pulse response data. A distinguishing feature of the Hankel matrix is that the elements of each anti-diagonal block are the same. By virtue of it's definition, the Hankel matrix can be expressed as the following product,
$$
M_{n}=L R,
$$
where
$$
\begin{aligned}
L & =\left[\begin{array}{c}
C \\
C A \\
C A^{2} \\
\vdots \\
C A^{n_{s}-1} \\
C A^{n_{s}} \\
\vdots \\
C A^{n-1}
\end{array}\right] \in \mathbf{R}^{m n \times n_{s}}, \\
R & =\left[\begin{array}{llllll}
B & A B & A^{2} B & \cdots & A^{n_{s}-1} B & A^{n_{s}} B
\end{array} \cdots A^{n-1} B\right] \in \mathbf{R}^{n_{s} \times n q}
\end{aligned}
$$

The observability and controllability matrices are embedded in $L$ and $R$, respectively, so long as $n \geq n_{s}$. Thus, we need to use at least $2 n_{s}$ data points to form $M_{n}$ and $\tilde{M}_{n}$ (defined below). We will assume we have enough data to meet this constraint. Because rank $\mathcal{O}_{n}=n_{s}$ and rank $\mathcal{C}_{n}=n_{s}$ then rank $L=n_{s}$ and rank $R=n_{s}$, thus,
$$
\operatorname{rank} M_{n}=n_{s},
$$

In other words, the rank of the Hankel matrix constructed from the pulse response is equal to the state dimension. This assumption on the rank not only requires that enough points be available so that the
column and row dimensions of $H_{n}$ are no less than $n_{s}$, but it also requires that certain "structural" properties of the system be satisfied -these details are discussed in the notes under the subjects of observability and controllability. If the state-space system undergoes a change of coordinates using $T \in \mathbf{R}^{n_{s} \times n_{s}}, \operatorname{det} T \neq 0$,
$$
\begin{aligned}
\mathbf{z}[k+1] & =T^{-1} A T \mathbf{z}[k]+T^{-1} B \mathbf{u}[k] \\
\mathbf{y}[k] & =C T \mathbf{z}[k]
\end{aligned}
$$

Note that $L$ and $R$ transform to
$$
L \mapsto \tilde{L}=L T, \quad R \mapsto \tilde{R}=T^{-1} R .
$$

Of course, the impulse response remains the same and so does the Hankel matrix $M_{n}=L R=\tilde{L} \tilde{R}$.
Now consider the situation in which the pulse response samples are provided, for example, by the procedure outlined in Sec. 2.2. In this case, $M_{n}$ can be formed from $\{h[k]\}, k=1,2, \ldots$, according to (11). Compute an SVD of $M_{n}$,
$$
M_{n}=U \Sigma V^{T}=\left[\begin{array}{ll}
U_{1} & U_{2}
\end{array}\right]\left[\begin{array}{cc}
\Sigma_{n_{s}} & 0 \\
0 & 0
\end{array}\right]\left[\begin{array}{l}
V_{1}^{T} \\
V_{2}^{T}
\end{array}\right]=U_{1} \Sigma_{n_{s}} V_{1}^{T},
$$
where $U_{1} \in \mathbf{R}^{m n \times n_{s}}, \Sigma_{n_{s}} \in \mathbf{R}^{n_{s} \times n_{s}}, V_{1} \in \mathbf{R}^{n q \times n_{s}}$. $L$ and $R$ can be determined from the SVD, however, the factorization is not unique, for example,
$$
\begin{aligned}
L & =U_{1} \text { and } R=\Sigma_{n_{s}} V_{1}^{T} \\
\text { or } L & =U_{1} \Sigma_{n_{s}} \text { and } R=V_{1}^{T} \\
\text { or } L & =U_{1} \Sigma_{n_{s}}^{\frac{1}{2}} \text { and } R=\Sigma_{n_{s}}^{\frac{1}{2}} V_{1}^{T} \\
\text { or } L & =U_{1} W \text { and } R=W^{-1} \Sigma_{n_{s}} V_{1}^{T}, \text { for any invertible } W .
\end{aligned}
$$

The choice of factorization just corresponds to a specific choice of coordinates in which the state-space matrices are to be expressed. Whatever factorization is used, $C$ is selected to be the first $m$ rows of $L$ and $B$ is selected to be first $q$ columns of $R$. To recover $A$, a new Hankel matrix is computed,
$$
\tilde{M}_{n}=\left[\begin{array}{ccccc}
h[2] & h[3] & h[4] & \cdots & h[n+1] \\
h[3] & h[4] & h[5] & \cdots & h[n+2] \\
h[4] & h[5] & h[6] & \cdots & h[n+3] \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
h[n+1] & h[n+2] & h[n+3] & \cdots & h[2 n]
\end{array}\right]=L A R .
$$

Once $L$ and $R$ have been determined from $M_{n}$ using the desired factorization, a left inverse of $L$, denoted $L^{\dagger}$, and a right inverse of $R$, denoted $R^{\dagger}$, are used to compute $A$ in the same coordinates as $C$ and $B$,
$$
A=L^{\dagger} \tilde{M}_{n} R^{\dagger}
$$

The left and right inverses from the pseudo-inverse expressions can be used. For example, the SVD of $M_{n}$ conveniently allows specification of left and right inverses using the matrices from the decomposition: if $L=U_{1}$ and $R=\Sigma_{n_{s}} V_{1}^{T}$, then $L^{\dagger}=U_{1}^{T}$ and $R^{\dagger}=V_{1} \Sigma_{n_{s}}^{-1}$.

In practice, the Hankel matrix is always full rank when using real data so the one way to estimate a model is to analyze the singular values of $M_{n}$ and make a judgement on a suitable model order. For
example, if there is a notable "jump" in the singular values (like in Fig. 6), then a sensible approach is to retain the largest singular values and truncate those after the "jump". Thus, although $M_{n}$ is full rank, it is approximated by a rank $n_{s}$ matrix, where $n_{s}$ is the selected model order. In other words, $n_{s}$ is not a priori specified -it is determined from analysis of the data, and once determined, $M_{n}$ is replaced by the "optimal" rank $n_{s}$ approximation
$$
M_{n} \approx U_{1} \Sigma_{n_{s}} V_{1}^{*}, \text { where } \Sigma_{n_{s}}=\left[\begin{array}{cccc}
\sigma_{1} & 0 & \cdots & 0 \\
0 & \sigma_{2} & & \\
\vdots & & \ddots & \\
0 & & & \sigma_{n_{s}}
\end{array}\right]
$$
where $U_{1}$ and $V_{1}$ represent the first $n_{s}$ columns of $U$ and $V$, respectively (where $U$ and $V$ are the unitary matrices from an SVD of $M_{n}$ ).

\subsection*{3.1 A note on the Gramian relations}

Recall the observability and controllability gramians using the first $n$ samples of the impulse response are defined as
$$
\begin{aligned}
G_{o}[n] & =\sum_{k=1}^{n}\left(A^{T}\right)^{k-1} C^{T} C A^{k-1}=L^{T} L \\
G_{c}[n] & =\sum_{k=1}^{n}(A)^{k-1} B B^{T}\left(A^{T}\right)^{k-1}=R R^{T}
\end{aligned}
$$

If we make the following choices for $L$ and $R$,
$$
L=U_{1} \Sigma_{n_{s}}^{\frac{1}{2}}, \quad R=\Sigma_{n_{s}}^{\frac{1}{2}} V_{1}^{T}
$$
then the gramians are balanced, in other words, $G_{o}[n]=G_{c}[n]=\Sigma_{n_{s}}$, because
$$
\begin{aligned}
G_{o}[n]=L^{T} L & =\left(U \Sigma_{n_{s}}^{\frac{1}{2}}\right)^{T} U \Sigma_{n_{s}}^{\frac{1}{2}}=\Sigma_{n_{s}} \\
G_{c}[n]=R R^{T} & =\left(\Sigma_{n_{s}}^{\frac{1}{2}} V^{T}\right)\left(\Sigma_{n_{s}}^{\frac{1}{2}} V^{T}\right)^{T}=\Sigma_{n_{s}}
\end{aligned}
$$

Now if the system is asymptotically stable we can take the limit $p \rightarrow \infty$, then
$$
\begin{align*}
& G_{o}[\infty]=\sum_{k=1}^{\infty}\left(A^{T}\right)^{k-1} C^{T} C A^{k-1}  \tag{12}\\
& G_{c}[\infty]=\sum_{k=1}^{\infty}(A)^{k-1} B B^{T}\left(A^{T}\right)^{k-1} \tag{13}
\end{align*}
$$
and these gramians satisfy the corresponding discrete-time Lyapunov equations,
$$
\begin{aligned}
A^{T} G_{o}[\infty] A-G_{o}[\infty] & =-C^{T} C \\
A G_{c}[\infty] A^{T}-G_{c}[\infty] & =-B B^{T} .
\end{aligned}
$$
![](https://cdn.mathpix.com/cropped/2024_11_25_befee36b63c0601e103eg-10.jpg?height=332&width=1050&top_left_y=379&top_left_x=581)

Figure 2: Block diagram associated with testing a connection of a continuous-time subsystems with three inputs and two outputs.

It's not possible to let $p \rightarrow \infty$ since only a finite amount is data is collected, however, if the Hankel matrix is constructed from essentially the entire transient pulse response data record, then for all practical purposes
$$
G_{o}[n] \approx G_{o}[\infty] \text { and } G_{c}[n] \approx G_{c}[\infty]
$$

Thus, these coordinates represent the traditional "balanced realization," however, it is not necessary to decompose the Hankel matrices so that balanced coordinates are used.

\section*{4 Project Tasks}

\subsection*{4.1 Background: Three-input/two-output system composed of subsystems}

Input-output data obtained by testing a three-input/two-output system with broadband random test signals can be downloaded from the course website. The system is constructed from four single-input/single-output "subsystems": two resonators and two low-pass filters. The subsystems are connected in a certain topology. You will determine a state-space representation of a discrete-time model that very closely reproduces the pulse response measurements. Then, analysis of the pole-zero structure of the model transfer functions will allow you to determine the connection topology. Other interesting properties of the system will also be analyzed. The block diagram is shown in Fig. 2.

\subsection*{4.1.1 Response to random inputs}

Data sets can be downloaded from the course website. The data sequences are associated with a sample period of $t_{s}=1 / 50$ second. The data sets are named
random_u1.mat
random_u2.mat
random_u3.mat
It is necessary to discuss individual channels of input and output so the following notation is adopted. The $k$ th input sample of $\mathbf{u}$ is denoted $\mathbf{u}[k]$ but since it is necessary to refer to the first, second or third channel of the input, the elements of $\mathbf{u}[k]$ are labeled as follows,
$$
\mathbf{u}[k]=\left[\begin{array}{l}
u_{1}[k] \\
u_{2}[k] \\
u_{3}[k]
\end{array}\right]
$$
and similarly for the output,
$$
\mathbf{y}[k]=\left[\begin{array}{l}
y_{1}[k] \\
y_{2}[k]
\end{array}\right]
$$

The system is tested input-by-input so that is why three data sets are given. In the Matlab code shown below the vector y11 represents a set of output samples $\left\{y_{1}[k]\right\}$, when $\left\{u_{1}[k]\right\}$ is white noise and $u_{2}=u_{3}=0$. The input variable in Matlab is u11. Similarly, y21 represents a set of output samples $\left\{y_{2}[k]\right\}$ for the same input. These data are stored in random_u1.mat. When the second input channel is used to test the system, in other words, $\left\{u_{2}[k]\right\}$ is white noise and $u_{1}=u_{3}=0$, then the corresponding outputs $\left\{y_{1}[k]\right\}$ and $\left\{y_{2}[k]\right\}$ are represented by the Matlab variables y 12 and y 22 , respectively and the input by variable u22. These data are stored in random_u2.mat. Finally, when the third input channel is used to test the system, in other words, $\left\{u_{3}[k]\right\}$ is white noise and $u_{1}=u_{2}=0$, then the corresponding outputs $\left\{y_{1}[k]\right\}$ and $\left\{y_{2}[k]\right\}$ are represented by the Matlab variables y13 and y23, respectively, and the input by the variable by u33. These data are stored in random_u3.mat. It is possible to test the system in which all input channels are "on", however, the non-parametric spectral estimation techniques described in Sec. 2.2 require more test data to produce low-variance estimates because for a given input, the other two inputs are "disturbances" whose effects are only removed by longer averaging. The following Matlab code loads the input-output data and graphs it. Note that only one input channel is "on" in each data set. In this problem, $m=3$ (two outputs) and $q=2$ (two inputs). This Matlab code produces Figs. 3, 4 and 5.
```
% load response data
load random_u1.mat
y11 = y1;
y21 = y2;
u11 = u1;
load random_u2.mat
y12 = y1;
y22 = y2;
u22 = u2;
load random_u3.mat
y13 = y1;
y23 = y2;
u33 = u3;
Ndat = length(u11);
t = [0:Ndat-1]*ts;
ax = [0 10 -6 6];
figure(1)
subplot(311)
plot(t,u11)
title('u_1 input')
grid on; axis(ax); legend('u_1')
subplot(312)
plot(t,y11)
grid on; axis(ax); legend('y_1')
subplot(313)
plot(t,y21)
```
```
grid on; axis(ax); legend('y_2')
xlabel('Time (s)')
figure(2)
subplot(311)
plot(t,u22)
title('u_2 input')
grid on; axis(ax); legend('u_2')
subplot(312)
plot(t,y12)
grid on; axis(ax); legend('y_1')
subplot(313)
plot(t,y22)
grid on; axis(ax); legend('y_2')
xlabel('Time (s)')
figure(3)
subplot(311)
plot(t,u33)
title('u_3 input')
grid on; axis(ax); legend('u_3')
subplot(312)
plot(t,y13)
grid on; axis(ax); legend('y_1')
subplot(313)
plot(t,y23)
grid on; axis(ax); legend('y_2')
xlabel('Time (s)')
```
![](https://cdn.mathpix.com/cropped/2024_11_25_befee36b63c0601e103eg-13.jpg?height=890&width=1020&top_left_y=352&top_left_x=523)

Figure 3: System response when $u_{1}$ is "white noise" and $u_{2}=u 3=0$. This is only a segment of the data -there is 250 seconds of data in each set. Also note that the data are given as individual points although Matlab is connecting points with lines.
![](https://cdn.mathpix.com/cropped/2024_11_25_befee36b63c0601e103eg-13.jpg?height=885&width=1020&top_left_y=1471&top_left_x=523)

Figure 4: System response when $u_{2}$ is "white noise" and $u_{1}=u_{3}=0$.
![](https://cdn.mathpix.com/cropped/2024_11_25_befee36b63c0601e103eg-14.jpg?height=892&width=1025&top_left_y=340&top_left_x=520)

Figure 5: System response when $u_{3}$ is "white noise" and $u_{1}=u_{2}=0$.

\section*{Task \#1: Empirical frequency response estimates}

Use the cpsd function in Matlab (see Sec. 2.2) to compute
- the auto-spectra of each input channel: $S_{u_{1} u_{1}}, S_{u_{2} u_{2}}, S_{u_{3} u_{3}}$
- the cross-spectra of each input channel: $S_{u_{2} u_{1}}, S_{u_{3} u_{1}}, S_{u_{3} u_{2}}$
- graph all six spectra in a log-log plot with axis([0.1 fnyq 1e-5 1e-2]) and a frequency unit of hertz (the Nyquist frequency is denoted fnyq and the magnitudes of the cross-spectra are graphed since they are complex-valued)

Answer the following:
1. Show that the inputs are "white noise" because their auto-spectra are roughly constant over frequency.
2. Show that there is little correlation between the input signals because the magnitude of the crossspectra are, on average, a magnitude smaller than the auto-spectra (more averaging, i.e. long data record, will further reduce the cross-spectra magnitudes)
3. Calculate the mean square value of each (time-domain) input sequence -these are the variances of the signals. Calculate the mean values of $S_{u_{1} u_{1}}, S_{u_{2} u_{2}}, S_{u_{3} u_{3}}$ and multiply by the signal sample rate of 50 Hz -these results should be very close to the variances obtained from the time-domain calculations.
4. Use the following notation for the requency response/transfer functions from individual input channels to individual output channels:
$$
\begin{aligned}
& H_{11}=\text { output } y_{1} \text { due to input } u_{1} \\
& H_{21}=\text { output } y_{2} \text { due to input } u_{1} \\
& H_{12}=\text { output } y_{1} \text { due to input } u_{2} \\
& H_{22}=\text { output } y_{2} \text { due to input } u_{2} \\
& H_{13}=\text { output } y_{1} \text { due to input } u_{3} \\
& H_{23}=\text { output } y_{2} \text { due to input } u_{3}
\end{aligned}
$$

Estimate the frequency responses as follows:
$$
\begin{aligned}
& H_{11}=S_{y_{1} u_{1}} / S_{u_{1} u_{1}} \\
& H_{21}=S_{y_{2} u_{1}} / S_{u_{1} u_{1}} \\
& H_{12}=S_{y_{1} u_{2}} / S_{u_{2} u_{2}} \\
& H_{22}=S_{y_{2} u_{2}} / S_{u_{2} u_{2}} \\
& H_{13}=S_{y_{1} u_{3}} / S_{u_{3} u_{3}} \\
& H_{23}=S_{y_{2} u_{3}} / S_{u_{3} u_{3}}
\end{aligned}
$$

Graph the magnitude of $H_{11}$ and $H_{21}$ in a single figure (all frequency response graphs should have magnitudes on log-log axes, frequency unit of hertz and axis([0.1 fnyq 1e-3 1e2])). Graph the phase of $H_{11}$ and $H_{21}$ in a single figure (all phase graphs should have linear-log axes, frequency unit of hertz and axis([0.1 fnyq -200 200])). These figures show how the system outputs respond to $u_{1}$.

In another two figures, graph the magnitude and phase of $H_{12}$ and $H_{22}$. These figures show how the system outputs respond to $u_{2}$. Finally, graph the magnitude and phase of $H_{13}$ and $H_{23}$ in a final set of figures. There should be a total of three figures showing the frequency response magnitudes and three figures showing the phases. These represent empirical frequency response estimates of the system.

\section*{Task \#2: Pulse response estimates}

The pulse responses of the system are simple to compute from the frequency response estimates. Pulse responses and frequency frequency responses are Fourier transform pairs, thus, the pulse response is estimated by computing the inverse Fourier transform of the corresponding frequency response. For discrete-time systems, the inverse fast Fourier transform algorithm can be applied to the frequency response estimates to generate pulse response estimates. The function to use in Matlab for this is ifft. It is necessary to create a time vector for the pulse response estimates. The pulse response estimate has exactly the same number of points as the frequency response from which it was derived -this is the nfft variable from Sec. 2.2. Since the first point in the pulse response estimate corresponds to $t=0$ (when the pulse is applied), the time vector is computed as $\mathrm{t}=[0: \mathrm{nfft}-1] * \mathrm{ts} ;$.

Let the pulse response corresponding to $H_{11}$ be denoted $h_{11}$ and so forth. Create the following figures for the report:
- Graph $h_{11}$ and $h_{21}$ in a single figure with axis([ $\begin{array}{lll}0 & 3 & -2\end{array} 3$ ））and using the subplot feature so that each signal is shown in a separate part of the graph.
- Graph $h_{12}$ and $h_{22}$ in a single figure with axis ( $\left.\begin{array}{llll}0 & 3 & -2 & 3\end{array}\right]$ ) and using the subplot feature so that each signal is shown in a separate part of the graph.
- Graph $h_{13}$ and $h_{23}$ in a single figure with axis ( $\begin{array}{lll}0 & 3 & -2\end{array} 3$ ））and using the subplot feature so that each signal is shown in a separate part of the graph.

The pulse responses are used to estimate a parametric model in the next section. The pulse response of the MIMO system is denoted by $h$, in other words,
$$
h=\left[\begin{array}{lll}
h_{11} & h_{12} & h_{13} \\
h_{21} & h_{22} & h_{23}
\end{array}\right]
$$

\section*{Task \#3: Hankel matrix analysis and parametric model}

The state dimension is not known, however, one objective of the project is to the determine a statespace discrete-time system that can replicate the observed data. Only asymptotically stable models are considered since it clear from the test data that the system we are testing must be asymptotically stable. Also note that when using data sets from physical systems that $M_{n}$ is invariably full rank because there is some noise in the measurement data and furthermore the system may be nonlinear even though it can be well-approximated with a linear model. Thus, although it appears that an $n$-dimensional model is required a lower rank approximation almost always produces a "better" model. For example, asymptotic stability of the identified model is not explicitly enforced and using high model dimensions can actually produce unstable models.

At the $k$ th sample time, $\mathbf{y}[k]$ data in Fig. 3 form the first column of $h[k] \in \mathbf{R}^{2 \times 3}$. Similarly, $\mathbf{y}[k]$ from Fig. 4 form the second column of $h[k]$ and $\mathbf{y}[k]$ from Fig. 5 form the third column of $h[k]$. The singular values of $M_{n}, n \in\{20,40,60,80\}$, are shown in Fig. 6 where there appear to be 8 dominant singular values regardless of the dimension of $M_{n}$ This suggests that a reasonable initial choice of model order is $n_{s}=8$. You will select a handful of model orders and derive the corresponding state-space models from the Hankel matrix factorization and then compare your models in a few different ways.
1. Construct $M_{25} \in \mathbf{R}^{50 \times 75}$ and graph its singular values (use the same axis type and range as Fig. 6). Compute models from $M_{25}$ with the following state dimensions: $n_{s} \in\{7,8,10,16\}$. There is no need to print out the state-space matrices since you will compute the model properties below. Calculate the max (abs (eig(A)) ) to confirm that all models of order $\{7,8,10\}$ are asymptotically stable but that order 16 is unstable (recall this is a discrete-time system so the eigenvalue condition is different than for a continuous-time system).
2. Simulate the impulse response of each model with order $\{7,8,10\}$ and compare it to the measurement data. For each of the model orders create the following figures
- Graph $h_{11}$ and $h_{21}$ in a single figure with axis ( $\begin{array}{lll}0 & 3 & -2\end{array} 3$ ]) and using the subplot feature so that each signal is shown in a separate part of the graph. Show the model result against the pulse response estimate.
![](https://cdn.mathpix.com/cropped/2024_11_25_befee36b63c0601e103eg-17.jpg?height=846&width=1042&top_left_y=344&top_left_x=512)

Figure 6: Singular values of $M_{20}$ (blue), $M_{40}$ (red), $M_{60}$ (yellow) and $M_{80}$ (purple). The first 20 singular values are shown for each case.
- Graph $h_{12}$ and $h_{22}$ in a single figure with axis ( $\left.\begin{array}{llll}0 & 3 & -2 & 3\end{array}\right]$ ) and using the subplot feature so that each signal is shown in a separate part of the graph. Show the model result against the pulse response estimate.
- Graph $h_{13}$ and $h_{23}$ in a single figure with axis ( $\left.\begin{array}{llll}0 & 3 & -2 & 3\end{array}\right]$ ) and using the subplot feature so that each signal is shown in a separate part of the graph. Show the model result against the pulse response estimate.

Which model order has an inferior reproduction of the pulse response data? The remaining models are essentially indistinguishable, though, and could be used as a valid model for the system. Note that even if the physical system is asymptotically stable, there is no guarantee that the model produced from this process will be asymptotically stable (as shown by model order 16).
3. Another way to compare the model to the data is to compare the model frequency response to the empirical frequency response obtained from the pulse response data. The model frequency response is given by
$$
C\left(e^{j \omega t_{s}} I-A\right)^{-1} B+D,
$$
where $t_{s}=1 / 50$ is the sample period in seconds. The frequency $\omega$ is in the interval $\left[0, \omega_{n y q}\right]$, where $\omega_{n y q}$ is the Nyquist frequency (equal to one half the sampling frequency, in other words, $\omega_{n y q}=2 \pi 25 \mathrm{rad} / \mathrm{s}$ ). Note that when evaluating the formula, $\omega$ should have units of radians-persecond, however, when graphing use the frequency unit of hertz. For each of the model orders create the following figures
- Graph the magnitude of $H_{11}$ and $H_{21}$ in a single figure with axis([0.1 fnyq 1e-3 1e2])). Compare the empirical frequency response to the model frequency response.
- Graph the phase of $H_{11}$ and $H_{21}$ in a single figure with axis([0.1 fnyq -200 200])). Compare the empirical frequency response to the model frequency response.
- Graph the magnitude of $H_{12}$ and $H_{22}$ in a single figure with axis ([0.1 fnyq 1e-3 1e2])). Compare the empirical frequency response to the model frequency response.
- Graph the phase of $H_{12}$ and $H_{22}$ in a single figure with axis([0.1 fnyq -200 200])). Compare the empirical frequency response to the model frequency response.
- Graph the magnitude of $H_{13}$ and $H_{23}$ in a single figure with axis ([0.1 fnyq 1e-3 1e2])). Compare the empirical frequency response to the model frequency response.
- Graph the phase of $H_{13}$ and $H_{23}$ in a single figure with axis([0.1 fnyq -200 200])). Compare the empirical frequency response to the model frequency response.

From the frequency response perspective, which model order has an inferior reproduction of the empirical frequency response? Which model orders yield almost indistinguishable frequency responses compared to the empirical data?

\section*{Task \#4: Transmission zeros of individual I/O channels}

The $n_{s}=\{8,10\}$ models provide very accurate reproduction of the pulse response and frequency response compared to the data. The transmission zeros of each of the six scalar transfer functions are considered for these models. A transmission zero for a SISO system occurs at $z \in \mathbf{C}$ when there exist a vector $\mathbf{x}_{0} \neq 0$ and a scalar $w \neq 0$ such that
$$
\left[\begin{array}{cc}
z I-A & -B  \tag{14}\\
C & D
\end{array}\right]\left[\begin{array}{c}
\mathbf{x}_{0} \\
w
\end{array}\right]=0
$$

Let the input be $u[k]=w z^{k}, k=0,1,2, \ldots$ (the input is scalar-valued because the SISO case is considered here). The state vector $\mathrm{x}_{k}=\mathbf{x}_{0} z^{k}$ satisfies the difference equation with this input because
$$
\mathbf{x}_{0} z^{k+1}=A \underbrace{\mathbf{x}_{0} z^{k}}_{\mathbf{x}[k]}+B \underbrace{x z^{k}}_{u[k]} \Longrightarrow(z I-A) \mathbf{x}_{0}-B w=0 .
$$

Furthermore, the output is zero for all samples because
$$
\mathbf{y}[k]=C \mathbf{x}_{0} z^{k}+D w z^{k}=\left[\begin{array}{ll}
C & D
\end{array}\right]\left[\begin{array}{c}
\mathbf{x}_{0} \\
w
\end{array}\right] z^{k}=0
$$

The zeros can be computed from a generalized eigenvalue problem,
$$
\left[\begin{array}{cc}
A & B \\
-C & -D
\end{array}\right]\left[\begin{array}{c}
\mathbf{x}_{0} \\
w
\end{array}\right]=z\left[\begin{array}{ll}
I & 0 \\
0 & 0
\end{array}\right]\left[\begin{array}{c}
\mathbf{x}_{0} \\
w
\end{array}\right]
$$

See the Matlab eig command for more information on solving this generalized eigenvalue problem. Answer the following:
1. In a figure graph the eigenvalues of the $n_{s}=8$ model in the complex plane (refer to the real and imag command in Matlab). Denote the eigenvalues with an " $x$ ". Draw a circle of radius 1 (use the Matlab command axis square so the aspect ratio is one-to-one) and note that if your model is asymptotically stable (it should be) then the eigenvalues will lie within the unit circle.
2. Graph the eigenvalues of the $n_{s}=10$ model in a new figure. Where are the extra eigenvalues located in the 10 state model?
3. For the $n_{s}=8$ model, create six separate figures showing the eigenvalues (marked with " $x$ " like in the previous figures) and the transmission zeros (marked with "o") calculated for each individual channel. The inf zeros can be ignored. Note that the eigenvalues will be the same in all cases, however, the transfer functions in each channel are different because the zeros are different. If a zero is in close proximity to an eigenvalue in a SISO transfer function then that eigenvalue is not a pole of the transfer function (the zero and eigenvalue effectively "cancel" each other). Carefully study each figure and note that although some zeros and poles cancel each other in certain channels, taken collectively, each eigenvalue will appear as a pole in at least one of the six SISO transfer functions.
4. Now consider the $n_{s}=10$ model. This model is over-parameterized in some sense because its pulse response and frequency response are essentially identical to those associated with the $n_{s}=8$ model yet it has two more states. Create another six figures of the eigenvalue and zero plots for each channel and show that the added eigenvalues are always accompanied by zeros in close proximity and that this occurs for all six channels. This creates an eigenvalue-zero cancellation in each channel so that the extra two eigenvalues in the $n_{s}=10$ case do not have any impact on the input-output properties of the system compared to the $n_{s}=8$ model. Thus, the $n_{s}=8$ model can be considered a more efficient representation of the system given the available data.

\section*{Task \#5: Block diagram derived from analysis of SISO channels}

The test data were generated by connecting four subsystems in a certain topology. Each although each subsystem is a circuit with many components, its dominant behavior is governed by a two state model. This is why $n_{s} \geq 8$ is required for an accurate representation of the input-output properties of the 3-input/2-output system. The subsystems consist of two resonator circuits and two low-pass filter circuits.

The $n_{s}=8$ model is used for the analysis in this task. A discrete-time eigenvalue $\lambda_{d}$ can be converted into its "equivalent" continuous-time eigenvalues according to $\lambda_{d}=e^{\lambda_{c} t_{s}}$, where $\lambda_{c}$ is the continuous-time eigenvalue. This is due to the fact that the " $A$ " matrix for a sampled linear continuoustime system is given by $e^{A_{c} t_{s}}$-see (7). Note that imaginary part of $\lambda_{c}$ is not uniquely defined, so just choose the smallest value for the imaginary part of $\lambda_{c}$ (this choice is justified as long as there is no aliasing of resonant frequencies during the data collection). Make a table showing the eigenvalues of the discrete-time model and their corresponding continuous-time equivalents. From the set of $\lambda_{c}$, show that there are two lightly damped resonators are in your model. What are their approximate natural frequencies in hertz? How do the frequencies compare with certain features in frequency response graphs?

The eigenvalues of each resonator are distinct (despite the fact that they are quite close) so give each complex conjugate pair of resonator eigenvalues a distinct label: "RES1" for the resonator with the slightly higher natural frequency and "RES2" for the other resonator. In addition to the two resonators, there are also two pairs of complex-conjugate eigenvalues. Each complex-conjugate pair is associated with a low-pass filter. Denote the more highly damped pair by "LP1" for "low-pass 1", and the other by "LP2".

You can use the following information to determine the connection topology of RES1, RES2, LP1 and LP2:
- Study which eigenvalues are not canceled by transmission zeros in the individual channels inputoutput channels (therefore, the subsystems with those eigenvalues as poles must be in the path from that given input to output).
- The low-frequency asymptotes of the frequency response magnitudes RES1 and RES2 are both "1". This information is required since two systems in a given input-output channel may be connected in series or in parallel (you need to justify which connection you make is the correct choice).
- Only three summing junctions are required to connect the subsystems.

Remark: The block diagram should consist of only the four blocks shown below and summing junctions. Each block is a single-input/single-output system so there should be only one arrow coming into, or leaving from, a block. In other words, arrange the blocks shown below with three summing junctions to "fill in" the "Continuous-time subsystems" block in Fig. 2 (the number of eigenvalues of each block are shown below for clarity but this can be omitted from your final block diagram):
![](https://cdn.mathpix.com/cropped/2024_11_25_befee36b63c0601e103eg-20.jpg?height=131&width=1551&top_left_y=1214&top_left_x=268)